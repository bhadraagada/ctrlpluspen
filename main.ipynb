{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc457e79",
   "metadata": {},
   "source": [
    "# üñäÔ∏è Handwriting Detection & OCR Pipeline - Implementation Plan\n",
    "\n",
    "## RTX 3050 Optimized (8GB VRAM) | Local Execution Only\n",
    "\n",
    "---\n",
    "\n",
    "## üìã MASTER IMPLEMENTATION PROMPT\n",
    "\n",
    "This notebook contains the complete implementation plan and instructions for building a handwriting detection and OCR pipeline. When code generation is requested, follow these specifications exactly.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. PROJECT OVERVIEW\n",
    "\n",
    "**Goal:** Build a two-stage pipeline:\n",
    "1. **Stage 1 - Detection:** YOLOv8 to detect handwritten text regions (words/lines)\n",
    "2. **Stage 2 - OCR:** TrOCR (or fallback EasyOCR) to recognize text in detected regions\n",
    "\n",
    "**Target Hardware:** NVIDIA RTX 3050 (8GB VRAM)\n",
    "**Execution:** 100% local (no Colab, no cloud)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. STRICT CONSTRAINTS CHECKLIST\n",
    "\n",
    "| # | Constraint | Implementation |\n",
    "|---|------------|----------------|\n",
    "| 1 | **ONLY real handwritten data** | IAM, ICDAR, Bentham - NO synthetic fonts |\n",
    "| 2 | **8GB VRAM limit** | imgsz=640, batch=2-8, AMP enabled |\n",
    "| 3 | **Local execution only** | No Colab, no Drive, local paths only |\n",
    "| 4 | **Deterministic conversion** | Scripted YOLO format conversion |\n",
    "| 5 | **Detection + OCR pipeline** | YOLOv8 ‚Üí TrOCR/EasyOCR |\n",
    "| 6 | **Checkpoint & resume** | Clear save/load instructions |\n",
    "| 7 | **Evaluation metrics** | mAP@0.5, mAP@0.5:0.95, CER, WER |\n",
    "| 8 | **CLI runnable** | `python script.py --config ...` |\n",
    "| 9 | **Quick-test mode** | 100 images, few epochs for validation |\n",
    "| 10 | **Troubleshooting** | OOM, CUDA, driver guidance |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a72285",
   "metadata": {},
   "source": [
    "## 3. RTX 3050 HYPERPARAMETER DEFAULTS\n",
    "\n",
    "### Critical GPU-Memory Settings\n",
    "\n",
    "```yaml\n",
    "# YOLO Detection Training Defaults\n",
    "imgsz: 640                    # Image size (reduce to 512 if OOM)\n",
    "batch: 4                      # Start with 4, reduce to 2 if OOM\n",
    "device: 0                     # GPU device ID\n",
    "amp: true                     # Mixed precision - CRITICAL for 3050\n",
    "workers: 4                    # DataLoader workers (reduce if RAM issues)\n",
    "cache: false                  # Don't cache images in RAM\n",
    "epochs: 50                    # Adjust based on dataset size\n",
    "patience: 10                  # Early stopping patience\n",
    "\n",
    "# Gradient Accumulation (simulate larger batch)\n",
    "accumulate: 4                 # Effective batch = batch * accumulate = 16\n",
    "\n",
    "# Memory-Critical Fallbacks\n",
    "# If OOM persists:\n",
    "#   1. batch: 2\n",
    "#   2. imgsz: 512\n",
    "#   3. workers: 2\n",
    "#   4. Close other GPU applications\n",
    "\n",
    "# OCR (TrOCR) Settings\n",
    "ocr_batch_size: 8             # Process crops in batches\n",
    "ocr_max_length: 128           # Max sequence length\n",
    "use_fp16: true                # Half precision inference\n",
    "```\n",
    "\n",
    "### Memory Estimation for RTX 3050 (8GB)\n",
    "| Config | Estimated VRAM Usage |\n",
    "|--------|---------------------|\n",
    "| YOLOv8n, batch=4, imgsz=640 | ~3-4 GB |\n",
    "| YOLOv8s, batch=2, imgsz=640 | ~4-5 GB |\n",
    "| TrOCR inference, batch=8 | ~2-3 GB |\n",
    "| Combined pipeline inference | ~4-5 GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf359d",
   "metadata": {},
   "source": [
    "## 4. DATASET REQUIREMENTS & ACCESS\n",
    "\n",
    "### ‚ö†Ô∏è CRITICAL: Real Handwritten Data ONLY\n",
    "\n",
    "**Allowed Datasets (in order of recommendation):**\n",
    "\n",
    "### A. IAM Handwriting Database (PRIMARY RECOMMENDATION)\n",
    "- **URL:** https://fki.tic.heia-fr.ch/databases/iam-handwriting-database\n",
    "- **Access:** FREE registration required\n",
    "- **Sign-up Steps:**\n",
    "  1. Go to https://fki.tic.heia-fr.ch/databases/iam-handwriting-database\n",
    "  2. Click \"Register\" and create an account\n",
    "  3. After email verification, request access to IAM dataset\n",
    "  4. Download: `words.tgz`, `lines.tgz`, `ascii.tgz` (annotations)\n",
    "- **Content:** ~115,000 word images from 657 writers\n",
    "- **Format:** Word/line-level segmented images with ground truth text\n",
    "- **Best for:** Both detection training and OCR\n",
    "\n",
    "### B. ICDAR Handwriting Recognition Datasets\n",
    "- **URL:** https://rrc.cvc.uab.es/\n",
    "- **Access:** Registration required\n",
    "- **Datasets:**\n",
    "  - ICDAR 2013 Handwriting Segmentation\n",
    "  - ICDAR 2017 Handwritten Document Recognition\n",
    "- **Content:** Document images with word/line bounding boxes\n",
    "\n",
    "### C. CVL Database\n",
    "- **URL:** https://cvl.tuwien.ac.at/research/cvl-databases/an-off-line-database-for-writer-retrieval-writer-identification-and-word-spotting/\n",
    "- **Access:** FREE, direct download\n",
    "- **Content:** 310 documents from 27 writers\n",
    "\n",
    "### D. Bentham Papers (HTR Dataset)\n",
    "- **URL:** https://transcriptorium.eu/datasets/bentham-collection/\n",
    "- **Access:** FREE\n",
    "- **Content:** Historical handwritten documents\n",
    "\n",
    "### Dataset Directory Structure (Expected)\n",
    "```\n",
    "./data/\n",
    "‚îú‚îÄ‚îÄ iam/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ words/           # Word images\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ lines/           # Line images  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ascii/           # Ground truth annotations\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ splits/          # train/val/test splits\n",
    "‚îú‚îÄ‚îÄ yolo_format/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ labels/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ data.yaml\n",
    "‚îî‚îÄ‚îÄ ocr_crops/           # Cropped regions for OCR training\n",
    "```\n",
    "\n",
    "### üö´ FORBIDDEN Data Sources\n",
    "- Font-rendered \"handwriting\" fonts\n",
    "- Synthetic text generators (SynthText, etc.)\n",
    "- Printed text datasets (unless for negative samples only)\n",
    "- AI-generated handwriting images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28266bf2",
   "metadata": {},
   "source": [
    "## 5. ENVIRONMENT SETUP SPECIFICATIONS\n",
    "\n",
    "### Required Software Versions (RTX 3050 Compatible)\n",
    "\n",
    "```bash\n",
    "# CUDA Compatibility for RTX 3050 (Ampere architecture)\n",
    "CUDA Toolkit: 11.8 or 12.1 (recommended)\n",
    "cuDNN: 8.6+ (compatible with CUDA version)\n",
    "NVIDIA Driver: 520+ (for CUDA 11.8) or 530+ (for CUDA 12.1)\n",
    "\n",
    "# Python Environment\n",
    "Python: 3.10.x (recommended) or 3.11.x\n",
    "```\n",
    "\n",
    "### Exact Package Versions (Tested)\n",
    "\n",
    "```bash\n",
    "# Create conda environment\n",
    "conda create -n handwriting python=3.10 -y\n",
    "conda activate handwriting\n",
    "\n",
    "# PyTorch with CUDA 11.8 (RTX 3050 compatible)\n",
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Alternative: CUDA 12.1\n",
    "# pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core packages\n",
    "pip install ultralytics==8.0.200          # YOLOv8\n",
    "pip install transformers==4.35.0          # TrOCR\n",
    "pip install easyocr==1.7.1                # Backup OCR\n",
    "pip install paddleocr==2.7.0.3            # Alternative OCR (optional)\n",
    "\n",
    "# Utilities\n",
    "pip install opencv-python==4.8.1.78\n",
    "pip install Pillow==10.1.0\n",
    "pip install matplotlib==3.8.0\n",
    "pip install pandas==2.1.1\n",
    "pip install numpy==1.24.3\n",
    "pip install tqdm==4.66.1\n",
    "pip install editdistance==0.6.2           # For CER/WER calculation\n",
    "pip install jiwer==3.0.3                  # WER calculation\n",
    "pip install pycocotools==2.0.7            # mAP calculation\n",
    "pip install pyyaml==6.0.1\n",
    "pip install albumentations==1.3.1         # Data augmentation\n",
    "```\n",
    "\n",
    "### Verify Installation Script\n",
    "```python\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ba604",
   "metadata": {},
   "source": [
    "## 6. ARTIFACTS TO BE PRODUCED\n",
    "\n",
    "### Checklist of Deliverables\n",
    "\n",
    "| # | Artifact | File | Description |\n",
    "|---|----------|------|-------------|\n",
    "| **A** | README | `README.md` | Complete setup guide, commands, troubleshooting |\n",
    "| **B1** | Environment Check | `scripts/check_environment.py` | Verify CUDA, GPU, packages |\n",
    "| **B2** | Dataset Downloader | `scripts/download_dataset.py` | Instructions + manifest checker |\n",
    "| **B3** | YOLO Converter | `scripts/convert_to_yolo.py` | IAM ‚Üí YOLO format conversion |\n",
    "| **B4** | Data Validator | `scripts/validate_dataset.py` | Check labels, visualize samples |\n",
    "| **B5** | Detection Trainer | `scripts/train_detector.py` | YOLOv8 training with RTX 3050 config |\n",
    "| **B6** | Inference Script | `scripts/run_inference.py` | Detect regions, save crops + JSON |\n",
    "| **B7** | OCR Runner | `scripts/run_ocr.py` | TrOCR/EasyOCR on crops |\n",
    "| **B8** | Post-processor | `scripts/postprocess.py` | Box ordering, line merging |\n",
    "| **B9** | Evaluator | `scripts/evaluate.py` | mAP, CER, WER computation |\n",
    "| **B10** | Full Pipeline | `scripts/predict.py` | End-to-end `predict(image_path)` ‚Üí JSON |\n",
    "| **B11** | Visualizer | `scripts/visualize.py` | Draw boxes + OCR overlay |\n",
    "| **C** | Quick Test | `scripts/quick_test.py` | 100 images, smoke test mode |\n",
    "| **D** | Config | `configs/rtx3050.yaml` | Optimized hyperparameters |\n",
    "| **E** | Notebook | `main.ipynb` | Interactive version with all cells |\n",
    "\n",
    "### Directory Structure (Final)\n",
    "```\n",
    "handwriting/\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ configs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ rtx3050.yaml          # RTX 3050 optimized config\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ quick_test.yaml       # Minimal config for testing\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ data.yaml             # YOLO dataset config\n",
    "‚îú‚îÄ‚îÄ scripts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ check_environment.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ download_dataset.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ convert_to_yolo.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ validate_dataset.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train_detector.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ run_inference.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ run_ocr.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ postprocess.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ predict.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ visualize.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ quick_test.py\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ iam/                  # Raw IAM dataset (user downloads)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ yolo_format/          # Converted YOLO format\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ detector/             # YOLO checkpoints\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ocr/                  # TrOCR cache\n",
    "‚îú‚îÄ‚îÄ outputs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ detections/           # Detection results\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ crops/                # Cropped regions\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ visualizations/       # Annotated images\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results/              # Evaluation metrics\n",
    "‚îî‚îÄ‚îÄ main.ipynb                # Interactive notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1282f",
   "metadata": {},
   "source": [
    "## 7. DETAILED IMPLEMENTATION SPECIFICATIONS\n",
    "\n",
    "### 7.1 Dataset Conversion (IAM ‚Üí YOLO)\n",
    "\n",
    "**Input:** IAM dataset structure with XML annotations\n",
    "**Output:** YOLO format with normalized coordinates\n",
    "\n",
    "```\n",
    "YOLO Label Format (per line):\n",
    "<class_id> <x_center> <y_center> <width> <height>\n",
    "\n",
    "Where:\n",
    "- class_id: 0 (single class: \"handwriting\")\n",
    "- All coordinates normalized to [0, 1] relative to image size\n",
    "```\n",
    "\n",
    "**Conversion Logic:**\n",
    "1. Parse IAM XML annotations for word/line bounding boxes\n",
    "2. Convert absolute pixel coords ‚Üí normalized coords\n",
    "3. Handle edge cases: boxes outside image bounds, zero-size boxes\n",
    "4. Split into train/val/test following IAM official splits\n",
    "5. Generate `data.yaml` for Ultralytics\n",
    "\n",
    "**Validation Checks:**\n",
    "- All labels have 5 values per line\n",
    "- All values in [0, 1] range\n",
    "- No empty label files\n",
    "- Image-label filename correspondence\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 YOLO Detection Training\n",
    "\n",
    "**Model:** YOLOv8n or YOLOv8s (nano/small for 3050)\n",
    "\n",
    "**Training Command Template:**\n",
    "```bash\n",
    "python train_detector.py \\\n",
    "    --data configs/data.yaml \\\n",
    "    --model yolov8n.pt \\\n",
    "    --imgsz 640 \\\n",
    "    --batch 4 \\\n",
    "    --epochs 50 \\\n",
    "    --device 0 \\\n",
    "    --amp \\\n",
    "    --project models/detector \\\n",
    "    --name handwriting_v1\n",
    "```\n",
    "\n",
    "**Resume Training:**\n",
    "```bash\n",
    "python train_detector.py --resume models/detector/handwriting_v1/weights/last.pt\n",
    "```\n",
    "\n",
    "**Key Ultralytics Parameters:**\n",
    "```python\n",
    "model.train(\n",
    "    data='configs/data.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=4,\n",
    "    device=0,\n",
    "    amp=True,                    # Mixed precision\n",
    "    workers=4,\n",
    "    patience=10,                 # Early stopping\n",
    "    save=True,\n",
    "    save_period=5,               # Checkpoint every 5 epochs\n",
    "    cache=False,                 # Don't cache (save RAM)\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3,\n",
    "    box=7.5,                     # Box loss gain\n",
    "    cls=0.5,                     # Class loss gain\n",
    "    dfl=1.5,                     # Distribution focal loss\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 OCR Integration\n",
    "\n",
    "**Primary: TrOCR (Microsoft)**\n",
    "```python\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Handwriting-specific model\n",
    "model_name = \"microsoft/trocr-base-handwritten\"\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU with half precision\n",
    "model = model.half().cuda()\n",
    "```\n",
    "\n",
    "**Fallback: EasyOCR**\n",
    "```python\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "```\n",
    "\n",
    "**OCR Decision Tree:**\n",
    "1. Try TrOCR first (better handwriting accuracy)\n",
    "2. If OOM: reduce batch size or use EasyOCR\n",
    "3. If TrOCR too slow: use EasyOCR for quick tests\n",
    "\n",
    "**TrOCR Fine-tuning Note (3050 Limitation):**\n",
    "- Full fine-tuning NOT recommended on 3050 (memory intensive)\n",
    "- Alternative: Use pretrained model OR fine-tune with:\n",
    "  - LoRA adapters (reduced memory)\n",
    "  - Frozen encoder, train decoder only\n",
    "  - Very small batch (1-2) with gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e085a",
   "metadata": {},
   "source": [
    "### 7.4 Post-Processing & Box Ordering\n",
    "\n",
    "**Box Ordering Algorithm:**\n",
    "```\n",
    "1. Sort boxes by y-coordinate (top to bottom)\n",
    "2. Group boxes into lines (y-overlap threshold)\n",
    "3. Within each line, sort by x-coordinate (left to right)\n",
    "4. Merge adjacent boxes if spacing < threshold\n",
    "5. Concatenate OCR results following order\n",
    "```\n",
    "\n",
    "**Line Detection Heuristic:**\n",
    "```python\n",
    "def group_into_lines(boxes, y_threshold=20):\n",
    "    \"\"\"Group boxes into text lines based on vertical overlap.\"\"\"\n",
    "    sorted_boxes = sorted(boxes, key=lambda b: b['y1'])\n",
    "    lines = []\n",
    "    current_line = [sorted_boxes[0]]\n",
    "    \n",
    "    for box in sorted_boxes[1:]:\n",
    "        if abs(box['y1'] - current_line[-1]['y1']) < y_threshold:\n",
    "            current_line.append(box)\n",
    "        else:\n",
    "            lines.append(sorted(current_line, key=lambda b: b['x1']))\n",
    "            current_line = [box]\n",
    "    lines.append(sorted(current_line, key=lambda b: b['x1']))\n",
    "    return lines\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7.5 Evaluation Metrics\n",
    "\n",
    "**Detection Metrics:**\n",
    "```python\n",
    "# Using Ultralytics built-in validation\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('path/to/best.pt')\n",
    "metrics = model.val(data='configs/data.yaml')\n",
    "\n",
    "# Key metrics:\n",
    "# - mAP@0.5: Mean Average Precision at IoU=0.5\n",
    "# - mAP@0.5:0.95: Mean AP averaged over IoU 0.5 to 0.95\n",
    "# - Precision, Recall, F1\n",
    "```\n",
    "\n",
    "**OCR Metrics:**\n",
    "```python\n",
    "from jiwer import wer, cer\n",
    "\n",
    "def calculate_metrics(predictions, ground_truths):\n",
    "    \"\"\"Calculate CER and WER for OCR results.\"\"\"\n",
    "    total_cer = cer(ground_truths, predictions)\n",
    "    total_wer = wer(ground_truths, predictions)\n",
    "    return {'CER': total_cer, 'WER': total_wer}\n",
    "\n",
    "# Expected ranges (good model):\n",
    "# CER: 5-15% (character error rate)\n",
    "# WER: 10-30% (word error rate)\n",
    "# Note: Handwriting is harder than printed text\n",
    "```\n",
    "\n",
    "**Expected Metrics on IAM (reference):**\n",
    "| Model | mAP@0.5 | CER | WER |\n",
    "|-------|---------|-----|-----|\n",
    "| YOLOv8n (detection) | 85-92% | - | - |\n",
    "| TrOCR (pretrained) | - | 8-15% | 20-35% |\n",
    "| TrOCR (fine-tuned) | - | 4-8% | 12-20% |\n",
    "\n",
    "---\n",
    "\n",
    "### 7.6 Output JSON Format\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image_path\": \"/path/to/image.jpg\",\n",
    "  \"image_size\": {\"width\": 1200, \"height\": 800},\n",
    "  \"detections\": [\n",
    "    {\n",
    "      \"id\": 0,\n",
    "      \"box\": [x1, y1, x2, y2],\n",
    "      \"confidence\": 0.92,\n",
    "      \"ocr_text\": \"Hello\",\n",
    "      \"ocr_confidence\": 0.87,\n",
    "      \"line_id\": 0\n",
    "    },\n",
    "    {\n",
    "      \"id\": 1,\n",
    "      \"box\": [x1, y1, x2, y2],\n",
    "      \"confidence\": 0.89,\n",
    "      \"ocr_text\": \"World\",\n",
    "      \"ocr_confidence\": 0.91,\n",
    "      \"line_id\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"lines\": [\n",
    "    {\"line_id\": 0, \"text\": \"Hello World\", \"boxes\": [0, 1]}\n",
    "  ],\n",
    "  \"aggregated_text\": \"Hello World\\nNext line here...\",\n",
    "  \"processing_time_ms\": 245\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906636b4",
   "metadata": {},
   "source": [
    "## 8. TROUBLESHOOTING FAQ\n",
    "\n",
    "### üî¥ Out of Memory (OOM) Errors\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "RuntimeError: CUDA out of memory. Tried to allocate X MiB\n",
    "torch.cuda.OutOfMemoryError\n",
    "```\n",
    "\n",
    "**Solutions (in order):**\n",
    "1. **Reduce batch size:** `--batch 2` or even `--batch 1`\n",
    "2. **Reduce image size:** `--imgsz 512` or `--imgsz 480`\n",
    "3. **Enable AMP:** Ensure `--amp` flag is set (uses FP16)\n",
    "4. **Gradient accumulation:** Set `accumulate=4` to simulate larger batch\n",
    "5. **Clear cache:** Add `torch.cuda.empty_cache()` between operations\n",
    "6. **Close other apps:** Check `nvidia-smi` for other GPU processes\n",
    "7. **Reduce workers:** `--workers 2` or `--workers 0`\n",
    "\n",
    "**For TrOCR specifically:**\n",
    "- Use `model.half()` for FP16 inference\n",
    "- Process crops in small batches (4-8)\n",
    "- Use EasyOCR as fallback (lighter)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ CUDA/Driver Issues\n",
    "\n",
    "**Check CUDA Status:**\n",
    "```bash\n",
    "# Check NVIDIA driver\n",
    "nvidia-smi\n",
    "\n",
    "# Check CUDA toolkit\n",
    "nvcc --version\n",
    "\n",
    "# Check PyTorch CUDA\n",
    "python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "python -c \"import torch; print(torch.version.cuda)\"\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "1. **`torch.cuda.is_available()` returns False:**\n",
    "   - Install CUDA-enabled PyTorch: \n",
    "     ```bash\n",
    "     pip uninstall torch torchvision torchaudio\n",
    "     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "     ```\n",
    "   - Verify driver supports CUDA version\n",
    "\n",
    "2. **CUDA version mismatch:**\n",
    "   - PyTorch CUDA version must be ‚â§ system CUDA version\n",
    "   - RTX 3050 supports CUDA 11.x and 12.x\n",
    "   - Recommended: CUDA 11.8 with PyTorch 2.1.0\n",
    "\n",
    "3. **Driver too old:**\n",
    "   - RTX 3050 needs driver ‚â• 470 (CUDA 11.4+)\n",
    "   - Recommended: driver 520+ for CUDA 11.8\n",
    "   - Update from: https://www.nvidia.com/drivers\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Dataset Access Problems\n",
    "\n",
    "**IAM Database:**\n",
    "- Registration required (free)\n",
    "- Account approval may take 24-48 hours\n",
    "- If denied: email administrators with research purpose\n",
    "\n",
    "**ICDAR datasets:**\n",
    "- Some competitions have closed access\n",
    "- Check https://rrc.cvc.uab.es/ for open datasets\n",
    "\n",
    "**Workaround:**\n",
    "- Use CVL database (direct download, no registration)\n",
    "- Use a small local dataset for pipeline validation\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Slow Training\n",
    "\n",
    "**Expected times on RTX 3050:**\n",
    "| Dataset Size | Epochs | Estimated Time |\n",
    "|--------------|--------|----------------|\n",
    "| 1,000 images | 50 | ~30-45 min |\n",
    "| 10,000 images | 50 | ~4-6 hours |\n",
    "| 50,000 images | 50 | ~20-30 hours |\n",
    "\n",
    "**Speed optimizations:**\n",
    "- Use `--cache ram` if you have 32GB+ RAM\n",
    "- Use `--workers 4` (not more, diminishing returns)\n",
    "- Use SSD for dataset storage\n",
    "- Disable validation during training: `--val false` (not recommended)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Poor Detection/OCR Results\n",
    "\n",
    "**Detection issues:**\n",
    "- Increase training epochs\n",
    "- Try YOLOv8s instead of YOLOv8n\n",
    "- Add data augmentation (already default in Ultralytics)\n",
    "- Check label quality: run `validate_dataset.py`\n",
    "\n",
    "**OCR issues:**\n",
    "- Ensure crops are not too small (min 32px height)\n",
    "- Add padding around crops (10-20%)\n",
    "- Try different TrOCR variants:\n",
    "  - `microsoft/trocr-base-handwritten` (general)\n",
    "  - `microsoft/trocr-large-handwritten` (better but heavier)\n",
    "- Preprocess crops: grayscale, contrast enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b1107",
   "metadata": {},
   "source": [
    "## 9. QUICK-TEST MODE SPECIFICATION\n",
    "\n",
    "### Purpose\n",
    "Validate the entire pipeline works on RTX 3050 before full training.\n",
    "\n",
    "### Quick-Test Parameters\n",
    "```yaml\n",
    "# Quick test config\n",
    "mode: quick_test\n",
    "num_samples: 100          # Only 100 images\n",
    "train_samples: 80\n",
    "val_samples: 20\n",
    "epochs: 5                 # Minimal epochs\n",
    "batch_size: 2             # Conservative\n",
    "imgsz: 640\n",
    "patience: 3               # Early stopping\n",
    "```\n",
    "\n",
    "### Expected Quick-Test Results\n",
    "| Metric | Expected Range | Notes |\n",
    "|--------|---------------|-------|\n",
    "| Training time | 5-10 min | YOLOv8n, 5 epochs |\n",
    "| mAP@0.5 | 30-60% | Low due to few samples |\n",
    "| Inference time | 50-100ms/image | Detection only |\n",
    "| OCR time | 100-200ms/crop | TrOCR batch=8 |\n",
    "| Total pipeline | 500-1000ms/image | Full page, ~10 words |\n",
    "\n",
    "### Quick-Test Command\n",
    "```bash\n",
    "python scripts/quick_test.py --samples 100 --epochs 5 --batch 2\n",
    "```\n",
    "\n",
    "### Success Criteria\n",
    "‚úÖ No OOM errors\n",
    "‚úÖ Training completes all epochs  \n",
    "‚úÖ Detection produces valid boxes\n",
    "‚úÖ OCR returns text strings\n",
    "‚úÖ JSON output is well-formed\n",
    "‚úÖ Visualization images are created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09633c12",
   "metadata": {},
   "source": [
    "## 10. FUTURE IMPROVEMENTS & NEXT STEPS\n",
    "\n",
    "### 10.1 Data Augmentation Strategies\n",
    "```python\n",
    "# Recommended augmentations for handwriting\n",
    "augmentations = {\n",
    "    'geometric': [\n",
    "        'RandomRotate(limit=5)',      # Slight rotation\n",
    "        'ShiftScaleRotate',           # Perspective changes\n",
    "        'ElasticTransform(alpha=50)', # Mimic writing variations\n",
    "    ],\n",
    "    'photometric': [\n",
    "        'RandomBrightnessContrast',\n",
    "        'GaussNoise',\n",
    "        'RandomGamma',\n",
    "        'CLAHE',                      # Contrast enhancement\n",
    "    ],\n",
    "    'handwriting_specific': [\n",
    "        'RandomErase',                # Simulate ink gaps\n",
    "        'Blur',                       # Simulate scan quality\n",
    "        'JpegCompression',            # Realistic artifacts\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 10.2 TrOCR Fine-tuning (When Resources Allow)\n",
    "\n",
    "**Option A: LoRA Fine-tuning (3050 Compatible)**\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Reduces trainable params by ~90%\n",
    "```\n",
    "\n",
    "**Option B: Decoder-only Fine-tuning**\n",
    "```python\n",
    "# Freeze encoder\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "# Only train decoder (smaller memory footprint)\n",
    "```\n",
    "\n",
    "### 10.3 End-to-End Alternatives (Research Directions)\n",
    "\n",
    "**A. TrOCR Direct (No Detection)**\n",
    "- For single-line images\n",
    "- Feed entire line to TrOCR\n",
    "- Simpler but requires pre-segmented data\n",
    "\n",
    "**B. DTROCR / Donut**\n",
    "- Document understanding transformers\n",
    "- No explicit detection step\n",
    "- Heavier, may not fit on 3050\n",
    "\n",
    "**C. PaddleOCR PP-OCRv4**\n",
    "- Integrated detection + recognition\n",
    "- Optimized for edge deployment\n",
    "- Good balance of speed/accuracy\n",
    "\n",
    "**D. Segment-then-Recognize (SAM + OCR)**\n",
    "- Use SAM for text region segmentation\n",
    "- Apply OCR to segments\n",
    "- Flexible but complex\n",
    "\n",
    "### 10.4 Resource Pointers\n",
    "\n",
    "| Topic | Resource |\n",
    "|-------|----------|\n",
    "| TrOCR Paper | https://arxiv.org/abs/2109.10282 |\n",
    "| YOLOv8 Docs | https://docs.ultralytics.com |\n",
    "| IAM Dataset | https://fki.tic.heia-fr.ch/databases/iam-handwriting-database |\n",
    "| HuggingFace TrOCR | https://huggingface.co/microsoft/trocr-base-handwritten |\n",
    "| PaddleOCR | https://github.com/PaddlePaddle/PaddleOCR |\n",
    "| Handwriting Recognition Survey | https://arxiv.org/abs/2112.07917 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab793e1",
   "metadata": {},
   "source": [
    "## 11. EXECUTION WORKFLOW\n",
    "\n",
    "### Step-by-Step Local Execution Guide\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    EXECUTION FLOWCHART                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Step 1: Environment Setup\n",
    "    ‚îú‚îÄ‚îÄ Create conda environment\n",
    "    ‚îú‚îÄ‚îÄ Install PyTorch + CUDA\n",
    "    ‚îú‚îÄ‚îÄ Install dependencies\n",
    "    ‚îî‚îÄ‚îÄ Run: python scripts/check_environment.py\n",
    "         ‚Üì\n",
    "Step 2: Dataset Preparation  \n",
    "    ‚îú‚îÄ‚îÄ Download IAM dataset (manual, requires registration)\n",
    "    ‚îú‚îÄ‚îÄ Place in ./data/iam/\n",
    "    ‚îú‚îÄ‚îÄ Run: python scripts/convert_to_yolo.py\n",
    "    ‚îî‚îÄ‚îÄ Run: python scripts/validate_dataset.py\n",
    "         ‚Üì\n",
    "Step 3: Quick Test (Recommended First)\n",
    "    ‚îî‚îÄ‚îÄ Run: python scripts/quick_test.py --samples 100 --epochs 5\n",
    "         ‚Üì\n",
    "Step 4: Full Training\n",
    "    ‚îú‚îÄ‚îÄ Run: python scripts/train_detector.py --config configs/rtx3050.yaml\n",
    "    ‚îî‚îÄ‚îÄ Monitor: tensorboard --logdir models/detector/\n",
    "         ‚Üì\n",
    "Step 5: Inference & OCR\n",
    "    ‚îú‚îÄ‚îÄ Run: python scripts/run_inference.py --source ./test_images/\n",
    "    ‚îî‚îÄ‚îÄ Run: python scripts/run_ocr.py --crops ./outputs/crops/\n",
    "         ‚Üì\n",
    "Step 6: Evaluation\n",
    "    ‚îú‚îÄ‚îÄ Run: python scripts/evaluate.py --predictions ./outputs/\n",
    "    ‚îî‚îÄ‚îÄ Review: ./outputs/results/metrics.json\n",
    "         ‚Üì\n",
    "Step 7: Full Pipeline Usage\n",
    "    ‚îî‚îÄ‚îÄ python scripts/predict.py --image ./my_handwritten_doc.jpg\n",
    "```\n",
    "\n",
    "### CLI Commands Summary\n",
    "\n",
    "```bash\n",
    "# 1. Environment check\n",
    "python scripts/check_environment.py\n",
    "\n",
    "# 2. Convert dataset\n",
    "python scripts/convert_to_yolo.py --input ./data/iam --output ./data/yolo_format\n",
    "\n",
    "# 3. Validate dataset  \n",
    "python scripts/validate_dataset.py --data ./data/yolo_format --visualize\n",
    "\n",
    "# 4. Quick test\n",
    "python scripts/quick_test.py --samples 100 --epochs 5\n",
    "\n",
    "# 5. Train detector\n",
    "python scripts/train_detector.py --config configs/rtx3050.yaml --epochs 50\n",
    "\n",
    "# 6. Resume training\n",
    "python scripts/train_detector.py --resume models/detector/handwriting_v1/weights/last.pt\n",
    "\n",
    "# 7. Run inference\n",
    "python scripts/run_inference.py --model models/detector/best.pt --source ./test_images/ --save-crops\n",
    "\n",
    "# 8. Run OCR\n",
    "python scripts/run_ocr.py --crops ./outputs/crops/ --model trocr --batch 8\n",
    "\n",
    "# 9. Evaluate\n",
    "python scripts/evaluate.py --det-results ./outputs/detections/ --ocr-results ./outputs/ocr/ --gt ./data/yolo_format/labels/val/\n",
    "\n",
    "# 10. Full pipeline\n",
    "python scripts/predict.py --image ./document.jpg --output ./result.json --visualize\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b022bf",
   "metadata": {},
   "source": [
    "## 12. IMPLEMENTATION SUMMARY & REMINDERS\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è CRITICAL REMINDERS FOR CODE GENERATION\n",
    "\n",
    "```\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üö® MANDATORY: REAL HANDWRITTEN DATA ONLY                        ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ IAM, ICDAR, CVL, Bentham = ‚úÖ ALLOWED                         ‚ïë\n",
    "‚ïë  ‚Ä¢ Synthetic fonts, printed text = ‚ùå FORBIDDEN                  ‚ïë\n",
    "‚ïë  ‚Ä¢ AI-generated handwriting = ‚ùå FORBIDDEN                       ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  If dataset unavailable: provide download instructions,          ‚ïë\n",
    "‚ïë  NOT synthetic alternatives                                      ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "```\n",
    "\n",
    "### GPU Memory Defaults (RTX 3050 - 8GB)\n",
    "\n",
    "| Parameter | Default | Fallback (OOM) | Aggressive |\n",
    "|-----------|---------|----------------|------------|\n",
    "| `imgsz` | 640 | 512 | 480 |\n",
    "| `batch` | 4 | 2 | 1 |\n",
    "| `workers` | 4 | 2 | 0 |\n",
    "| `amp` | True | True | True |\n",
    "| `accumulate` | 4 | 8 | 16 |\n",
    "| `cache` | False | False | False |\n",
    "\n",
    "### Key File Paths (Configurable)\n",
    "\n",
    "```python\n",
    "# All paths should be configurable via CLI args or env vars\n",
    "DATA_ROOT = os.getenv('HW_DATA_ROOT', './data')\n",
    "MODEL_ROOT = os.getenv('HW_MODEL_ROOT', './models')\n",
    "OUTPUT_ROOT = os.getenv('HW_OUTPUT_ROOT', './outputs')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ READY FOR CODE GENERATION\n",
    "\n",
    "This implementation plan is complete. When you request the actual code, I will generate:\n",
    "\n",
    "1. **All Python scripts** in `./scripts/` directory\n",
    "2. **Configuration files** in `./configs/` directory  \n",
    "3. **README.md** with full documentation\n",
    "4. **requirements.txt** with exact versions\n",
    "5. **Interactive notebook cells** for step-by-step execution\n",
    "\n",
    "### To generate the code, say:\n",
    "> \"Generate the complete implementation code following this plan\"\n",
    "\n",
    "or for specific parts:\n",
    "> \"Generate only the dataset conversion script\"\n",
    "> \"Generate only the training script\"\n",
    "> \"Generate the full pipeline predict function\"\n",
    "\n",
    "---\n",
    "\n",
    "*Plan created: December 2024*\n",
    "*Target: NVIDIA RTX 3050 (8GB VRAM)*\n",
    "*Primary dataset: IAM Handwriting Database*\n",
    "*Architecture: YOLOv8 Detection ‚Üí TrOCR Recognition*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handwriting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
